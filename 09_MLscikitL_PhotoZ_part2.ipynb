{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = figs/logos/logo-IJCLab_v1.png height=40, style=\"padding: 10px\"> \n",
    "<b>PhotoZ estimation with scikit learn Machine learning </b> <br>\n",
    "Last verified to run on 2022-03-21 with LSST Science Pipelines release w_2021_49 <br>\n",
    "Contact authors: Sylvie Dagoret-Campagne (DP0 Delegate) <br>\n",
    "Target audience: DP0 delegates member <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit:** Originally developed by Sylvie Dagoret-Campagne in the framework provided by Rubin DP0.1 (reference DP0.1 tutorials)\n",
    "\n",
    "Acknowledgement: Melissa Graham, Leanne Guy, Alex Drlica-Wagner, Keith Bechtol, Grzegorz Madejski, Louise Edwards, and many others .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives : Find hyperparameters for optimal of scikit-learn PhotoZ estimator.\n",
    "\n",
    "\n",
    "- Validation curves,\n",
    "- Leaning curves,\n",
    "- Bias and overfitting trade-off\n",
    "- grid search cross-validation\n",
    "- randoom search cross-validation\n",
    "\n",
    "\n",
    "Note in the case of a live demo dedicated to PhotoZ, for which the time is limited, the Section 1 must be skipped. A selection flag  FLAG_SHOW_PHOTOMETRY_DETECTION = False is set accordingly. \n",
    "\n",
    "\n",
    "**Note:** : \n",
    "-All plots are made with Holoview.\n",
    "- **Better select the maximum of CPU (4 CPU on RSP)**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general python packages\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pandas.testing import assert_frame_equal\n",
    "import os\n",
    "import errno\n",
    "import shutil\n",
    "import getpass\n",
    "import datetime\n",
    "# Import the Rubin TAP service utilities\n",
    "from lsst.rsp import get_tap_service, retrieve_query\n",
    "\n",
    "# LSST Science Pipelines (Stack) packages\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "import lsst.geom as geom\n",
    "import lsst.afw.coord as afwCoord\n",
    "afwDisplay.setDefaultBackend('matplotlib')\n",
    "\n",
    "#\n",
    "from lsst import skymap\n",
    "\n",
    "# Astropy\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.units.quantity import Quantity\n",
    "from astropy.visualization import (MinMaxInterval, SqrtStretch,ZScaleInterval,PercentileInterval,\n",
    "                                   ImageNormalize,imshow_norm)\n",
    "from astropy.visualization.stretch import SinhStretch, LinearStretch,AsinhStretch,LogStretch\n",
    "\n",
    "\n",
    "# Bokeh for interactive visualization\n",
    "import bokeh\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource, CDSView, GroupFilter, HoverTool\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import streams, opts\n",
    "from holoviews.operation.datashader import rasterize\n",
    "from holoviews.operation.datashader import datashade, dynspread\n",
    "from holoviews.plotting.util import process_cmap\n",
    "\n",
    "import datashader as dsh\n",
    "\n",
    "\n",
    "# Set the maximum number of rows to display from pandas\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "\n",
    "# Set the holoviews plotting library to be bokeh\n",
    "# You will see the holoviews + bokeh icons displayed when the library is loaded successfully\n",
    "#hv.extension('bokeh')\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "\n",
    "# Display bokeh plots inline in the notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What versions of bokeh and holoviews nd datashader are we working with?\n",
    "# This is important when referring to online documentation as\n",
    "# APIs can change between versions.\n",
    "print(\"Bokeh version: \" + bokeh.__version__)\n",
    "print(\"Holoviews version: \" + hv.__version__)\n",
    "print(\"Datashader version: \" + dsh.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  What version of the Stack are we using?\n",
    "! echo $IMAGE_DESCRIPTION\n",
    "! eups list -s | grep lsst_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow for matplotlib to create inline plots in our notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt      # imports matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "import warnings                      # imports the warnings library\n",
    "import gc                            # imports python's garbage collector\n",
    "\n",
    "# Ignore warnings\n",
    "from astropy.units import UnitsWarning\n",
    "warnings.simplefilter(\"ignore\", category=UnitsWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some plotting defaults:\n",
    "\n",
    "params = {'axes.labelsize': 28,\n",
    "          'font.size': 24,\n",
    "          'legend.fontsize': 14,\n",
    "          'xtick.major.width': 3,\n",
    "          'xtick.minor.width': 2,\n",
    "          'xtick.major.size': 12,\n",
    "          'xtick.minor.size': 6,\n",
    "          'xtick.direction': 'in',\n",
    "          'xtick.top': True,\n",
    "          'lines.linewidth': 3,\n",
    "          'axes.linewidth': 3,\n",
    "          'axes.labelweight': 3,\n",
    "          'axes.titleweight': 3,\n",
    "          'ytick.major.width': 3,\n",
    "          'ytick.minor.width': 2,\n",
    "          'ytick.major.size': 12,\n",
    "          'ytick.minor.size': 6,\n",
    "          'ytick.direction': 'in',\n",
    "          'ytick.right': True,\n",
    "          'figure.figsize': [18, 10],\n",
    "          'figure.facecolor': 'White'\n",
    "          }\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools to explain the expected detected magnitudes distributions\n",
    "\n",
    "- taken from from https://github.com/ixkael/Photoz-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(m) = m^\\alpha \\exp\\left( - \\left(m/m_{max}\\right)^\\beta \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_mag(imag_grid,maglim):\n",
    "    \"\"\"   \n",
    "    \n",
    "    Model of magnitude distribution in photometric survey\n",
    "    \n",
    "    p_mag(imag_grid,maglim)\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \n",
    "    input args:\n",
    "     - imag_grid : magnitude\n",
    "     - maglim limit of magnitude\n",
    "     \n",
    "     return the probability of magnitude distribution\n",
    "     \n",
    "     THIS IS THE MODEL THAT MUST BE USED\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # some parameters for prob(imagnitude)\n",
    "    alpha = 15.0 \n",
    "    beta = 2\n",
    "    off=1\n",
    "\n",
    "    # prob(imagnitude) distribution\n",
    "    p_imag = imag_grid**alpha*np.exp(-(imag_grid/(maglim-off))**beta)\n",
    "    p_imag /= p_imag.sum()\n",
    "    return p_imag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imag errir distribution as function of mag limit, as in Rykoff et al\n",
    "def imag_err(m, mlim):\n",
    "    \"\"\"\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \"\"\"\n",
    "    \n",
    "    a, b = 4.56, 1\n",
    "    k = 1\n",
    "    sigmadet = 5\n",
    "    teff = np.exp(a + b * (mlim - 21.))\n",
    "    F = 10**(-0.4*(m-22.5))\n",
    "    Flim = 10**(-0.4*(mlim-22.5))\n",
    "    Fnoise = (Flim/sigmadet)**2 * k * teff - Flim\n",
    "    return 2.5/np.log(10) * np.sqrt( (1 + Fnoise/F) / (F*k*teff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def det_prob(imag_grid,maglim):\n",
    "    \"\"\"\n",
    "    det_prob(imag_grid,maglim)\n",
    "    \n",
    "    Give detection probability of a magnitude\n",
    "    from https://github.com/ixkael/Photoz-tools\n",
    "    \n",
    "    input arg:\n",
    "    - imag_grid : magnitude grid\n",
    "    - maglim limit of magnitude\n",
    "    \n",
    "    return histogram of magnitude probability\n",
    "    \n",
    "    THIS IS THE MODEL THAT MUST BE USED\n",
    "    \n",
    "    \"\"\"\n",
    "    pp_mag=p_mag(imag_grid,maglim)\n",
    "    \n",
    "    detprob = 1*pp_mag \n",
    "    ind = (imag_grid >= maglim - 0.4)\n",
    "    #detprob[ind] *= ( 1 - scipy.special.erf((imag_grid[ind]-maglim+0.4)/0.4) )\n",
    "    # detection probability looks like a sigmoid\n",
    "    detprob[ind] *= np.exp( -0.5*((imag_grid[ind]-maglim+0.4)/0.2)**2)\n",
    "    detprob /= detprob.sum() * (imag_grid[1]-imag_grid[0])\n",
    "    return detprob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations and initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holoview Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_CURVE_SINGLE_WIDTH  = 400\n",
    "HV_CURVE_SINGLE_HEIGHT = 350\n",
    "HV_CURVE_MULTI_WIDTH  = 300\n",
    "HV_CURVE_MULTI_HEIGHT = 300\n",
    "HV_CURVE_MULTI_FRAME_WIDTH = 300\n",
    "HV_CURVE_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBINS_HISTO = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_HISTO_SINGLE_WIDTH  = 600\n",
    "HV_HISTO_SINGLE_HEIGHT = 600\n",
    "HV_HISTO_MULTI_WIDTH  = 300\n",
    "HV_HISTO_MULTI_HEIGHT = 300\n",
    "HV_HISTO_MULTI_FRAME_WIDTH = 300\n",
    "HV_HISTO_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HV_IMAGE_SINGLE_WIDTH  = 400\n",
    "HV_IMAGE_SINGLE_HEIGHT = 400\n",
    "HV_IMAGE_SINGLE_FRAME_WIDTH = 600\n",
    "HV_IMAGE_MULTI_WIDTH  = 400\n",
    "HV_IMAGE_MULTI_HEIGHT = 400\n",
    "HV_IMAGE_MULTI_FRAME_WIDTH = 300\n",
    "HV_IMAGE_MULTI_COLS   = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username\n",
    "myusername=getpass.getuser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary folders if necessary\n",
    "NBDIR       = 'photoz_part1'                           # relative path for this notebook output\n",
    "TMPTOPDIR   = \"/scratch\"                               # always write some output in /scratch, never in user HOME \n",
    "TMPUSERDIR  = os.path.join(TMPTOPDIR,myusername)       # defines the path of user outputs in /scratch \n",
    "TMPNBDIR    = os.path.join(TMPUSERDIR,NBDIR)           # output path for this particular notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create user temporary directory\n",
    "if not os.path.isdir(TMPUSERDIR):\n",
    "    try:\n",
    "        os.mkdir(TMPUSERDIR)\n",
    "    except:\n",
    "        raise OSError(f\"Can't create destination directory {TMPUSERDIR}!\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create this notebook temporary directory\n",
    "if not os.path.isdir(TMPNBDIR):\n",
    "    try:\n",
    "        os.mkdir(TMPNBDIR)\n",
    "    except:\n",
    "        raise OSError(f\"Can't create destination directory {TMPNBDIR}!\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Defines steering flags and parameters\n",
    "\n",
    "The Output queries may be saved in files if requested. \n",
    "By defaults all the following flags are set False : no query output is saved in file.\n",
    "To speed-up the demo, the presenter may keep some of those flags True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_WRITE_DATAFRAMEONDISK  = True                     # Select of query output will be saved on disk\n",
    "FLAG_READ_DATAFRAMEFROMDISK = True                     # Select of the query can be red from disk if it exists\n",
    "FLAG_CLEAN_DATAONDISK       = False                     # Select of the output queries saved in file will be cleaned at the end of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieve source from catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coadds-Magnitude cutoff for 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAX,GMAX,RMAX,IMAX,ZMAX,YMAX = 26.1, 27.4, 27.5, 26.8, 26.1,24.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection function in catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build a query passing a coordinate and a search radius\n",
    "def getQueryCircle(c: SkyCoord, r: Quantity) -> str:\n",
    "    query = \"SELECT obj.ra, obj.dec, obj.objectId, obj.extendedness, \"\\\n",
    "            \"obj.mag_u_cModel, obj.mag_g_cModel, obj.mag_r_cModel, \"\\\n",
    "            \"obj.mag_i_cModel, obj.mag_z_cModel, obj.mag_y_cModel, \"\\\n",
    "            \"obj.magerr_u_cModel, obj.magerr_g_cModel, obj.magerr_r_cModel, \"\\\n",
    "            \"obj.magerr_i_cModel, obj.magerr_z_cModel, obj.magerr_y_cModel, \"\\\n",
    "            \"truth.truth_type, truth.redshift, truth.match_objectId \" \\\n",
    "            \"FROM dp01_dc2_catalogs.object as obj \" \\\n",
    "            \"JOIN dp01_dc2_catalogs.truth_match as truth \" \\\n",
    "            \"ON truth.match_objectId = obj.objectId \" \\\n",
    "            \"WHERE CONTAINS(POINT('ICRS', obj.ra, obj.dec),\"\\\n",
    "            \"CIRCLE('ICRS', \" + str(c.ra.value) + \", \" + str(c.dec.value) + \", \" \\\n",
    "            + str(r.to(u.deg).value) + \" )) = 1 \" \\\n",
    "            \"AND obj.good = 1 \"  \\\n",
    "            \"AND truth.match_objectid >= 0 \" \\\n",
    "            \"AND truth.is_good_match = 1 \" \\\n",
    "            \"AND truth.truth_type = 1 \" \\\n",
    "            \"AND obj.mag_u_cModel - 5*obj.magerr_u_cModel < \" +str(UMAX) +\" \"\\\n",
    "            \"AND obj.mag_g_cModel - 5*obj.magerr_g_cModel < \" +str(GMAX) +\" \"\\\n",
    "            \"AND obj.mag_r_cModel - 5*obj.magerr_r_cModel < \" +str(RMAX) +\" \"\\\n",
    "            \"AND obj.mag_i_cModel - 5*obj.magerr_i_cModel < \" +str(IMAX) +\" \"\\\n",
    "            \"AND obj.mag_z_cModel - 5*obj.magerr_z_cModel < \" +str(ZMAX) +\" \"\\\n",
    "            \"AND obj.mag_y_cModel - 5*obj.magerr_y_cModel < \" +str(YMAX)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and initialisation of the Rubin TAP Service client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an instance of the TAP service\n",
    "service = get_tap_service()\n",
    "assert service is not None\n",
    "assert service.baseurl == \"https://data.lsst.cloud/api/tap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reference position on the sky  for a square seach\n",
    "c1 = SkyCoord(ra=62.0*u.degree, dec=-40.*u.degree, frame='icrs')\n",
    "size = 1.0 * u.deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = getQueryCircle(c1, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_result=f'cat_photozpart1_result.pkl'\n",
    "fullfilename_result=os.path.join(TMPNBDIR,filename_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection flags\n",
    "\n",
    "- put boolean flags here to avoid execution of some sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots on redshift distribution\n",
    "FLAG_SHOW_TRUE_REDSHIFT_DISTRIB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show plots to check the photometry selected for photoz\n",
    "# For a pure demo on photoZ, this section can be skipped\n",
    "FLAG_SHOW_PHOTOMETRY_DETECTION = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To speed up the database query, especially if you run this notebook in the context of a DP0 demo public session,\n",
    "you can copy my data file **cat_photozpart1_result.pkl** from **/scratch/sylvielsstfr/photoz_part1/cat_photozpart1_result.pkl** to your path **/scratch/yourusername/photoz_part1**\n",
    "\n",
    "- check the variable **FLAG_READ_DATAFRAMEFROMDISK=True**\n",
    "- your username is given by **yourusername=getpass.getuser()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_READ_DATAFRAMEFROMDISK and os.path.exists(fullfilename_result):\n",
    "    sql_result = pd.read_pickle(fullfilename_result)\n",
    "else:\n",
    "    job = service.submit_job(query)\n",
    "    job.run()\n",
    "    job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "    print('Job phase is', job.phase)\n",
    "    #sql_result = job.fetch_result().to_table().to_pandas()\n",
    "    \n",
    "    \n",
    "    # Create and submit the job. This step does not run the query yet\n",
    "    job = service.submit_job(query)\n",
    "    # Get the job URL\n",
    "    print('Job URL is', job.url)\n",
    "\n",
    "    # Get the job phase. It will be pending as we have not yet started the job\n",
    "    print('Job phase is', job.phase)\n",
    "    \n",
    "    # Run the job. You will see that the the cell completes executing,\n",
    "    # even though the query is still running\n",
    "    job.run()\n",
    "    \n",
    "    # Use this to tell python to wait for the job to finish if\n",
    "    # you don't want to run anything else while waiting\n",
    "    # The cell will continue executing until the job is finished\n",
    "    job.wait(phases=['COMPLETED', 'ERROR'])\n",
    "    print('Job phase is', job.phase)\n",
    "    \n",
    "    # A usefull funtion to raise an exception if there was a problem with the query\n",
    "    job.raise_if_error()\n",
    "    \n",
    "    # Once the job completes successfully, you can fetch the results\n",
    "    async_data = job.fetch_result()\n",
    "    \n",
    "    sql_result = async_data.to_table().to_pandas()\n",
    "    \n",
    "    \n",
    "if FLAG_WRITE_DATAFRAMEONDISK:\n",
    "    sql_result.to_pickle(fullfilename_result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sql_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for shorter names\n",
    "data.rename(columns={\"mag_u_cModel\": \"mag_u\", \"mag_g_cModel\": \"mag_g\",\"mag_r_cModel\": \"mag_r\",\n",
    "                     \"mag_i_cModel\": \"mag_i\", \"mag_z_cModel\": \"mag_z\",\"mag_y_cModel\": \"mag_y\",\n",
    "                     \"magerr_u_cModel\": \"magerr_u\", \"magerr_g_cModel\": \"magerr_g\",\"magerr_r_cModel\": \"magerr_r\",\n",
    "                     \"magerr_i_cModel\": \"magerr_i\", \"magerr_z_cModel\": \"magerr_z\",\"magerr_y_cModel\": \"magerr_y\",\n",
    "                    },inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output directory where the extracted catalog is temporary saved\n",
    "! ls -l $TMPNBDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map truth_type\n",
    "data['truth_type']=data['truth_type'].map({1: 'galaxy', 2: 'star', 3: 'SNe'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop objects that are not galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop objects that are not galaxies\n",
    "data.drop(data.loc[data['truth_type'] != 'galaxy' ].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NA\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "data[\"umg\"]=data[\"mag_u\"]- data[\"mag_g\"]\n",
    "data[\"gmr\"]=data[\"mag_g\"]- data[\"mag_r\"]\n",
    "data[\"rmi\"]=data[\"mag_r\"]- data[\"mag_i\"]\n",
    "data[\"imz\"]=data[\"mag_i\"]- data[\"mag_z\"]\n",
    "data[\"zmy\"]=data[\"mag_z\"]- data[\"mag_y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ckeck input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshifts distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_TRUE_REDSHIFT_DISTRIB:\n",
    "    (z_bin, count) = np.histogram(data.redshift, bins=NBINS_HISTO)\n",
    "    z_distribution = hv.Histogram(z_bin, count).opts(title=f\"redshift distribution\",color='darkblue', \n",
    "    xlabel='redshift', fontscale=1.2,\n",
    "    height=HV_HISTO_SINGLE_HEIGHT-100, width=HV_HISTO_SINGLE_WIDTH,tools=['hover'])\n",
    "    \n",
    "    z_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitudes distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle of photodetection\n",
    "\n",
    "Study of a simple model to find the expected magnitude distribution including photodetection bias:\n",
    "\n",
    "- blue curve : the true magnitude distribution\n",
    "- green curve : the detected magnitude distribution\n",
    "- red : detection threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymag_grid = np.linspace(17,30,100)\n",
    "mymaglim = 27\n",
    "mymag0 = 17\n",
    "mymag1 = 30\n",
    "prob_mag_tab = p_mag(mymag_grid,mymaglim)\n",
    "dens_mag_tab = prob_mag_tab/np.sum(prob_mag_tab)/(mymag_grid[1]-mymag_grid[0])\n",
    "det_prob_tab =  det_prob(mymag_grid,mymaglim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_opts = dict(\n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                height=HV_CURVE_SINGLE_HEIGHT, width=HV_CURVE_SINGLE_WIDTH+100,tools=['hover']\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    curve_magdist = hv.Curve(zip(mymag_grid,dens_mag_tab),label=\"true mag\").opts(**curve_opts).opts(color=\"blue\") \n",
    "    curve_maglim = hv.VLine(mymaglim,label=\"detection threshold\").opts(color=\"red\")\n",
    "    curve_prob = hv.Curve(zip(mymag_grid,det_prob_tab),label=\"detected mag\").opts(**curve_opts).opts(color=\"green\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = (curve_magdist * curve_prob * curve_maglim).opts(legend_position='top_left',xlabel=\"magnitude\",ylabel=\"probability\",title=\"magnitude density distribution\")\n",
    "    layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magnitude distribution before cutoff at detection threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The catalog sources have been extracted $5\\sigma$ above the cutoff. This allow the user to refine the expected cutoff inside each band "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection cutiff definition\n",
    "maglim_u = hv.VLine(UMAX).opts(color=\"magenta\")\n",
    "maglim_g = hv.VLine(GMAX).opts(color=\"magenta\")\n",
    "maglim_r = hv.VLine(RMAX).opts(color=\"magenta\")\n",
    "maglim_i = hv.VLine(IMAX).opts(color=\"magenta\")\n",
    "maglim_z = hv.VLine(ZMAX).opts(color=\"magenta\")\n",
    "maglim_y = hv.VLine(YMAX).opts(color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histo_opts = dict(\n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                height=HV_HISTO_MULTI_HEIGHT, width=HV_HISTO_MULTI_WIDTH,tools=['hover']\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magu = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magg = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magr = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magi = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magz = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magy = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout1 = h_magu * maglim_u + h_magg * maglim_g + h_magr * maglim_r + h_magi *  maglim_i + h_magz *  maglim_z + h_magy *  maglim_y\n",
    "    layout1.cols(HV_HISTO_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Photometric detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The magnitudes in DC2 correcpond to average magnitudes and their photometric error. In the telescope, the detection correspond to the realisation of one sample of these distribution.\n",
    "Then the cutoff will be applied on detected magnitudes. (But the evaluation of photoz algorithm will be applied on the average magnitudes in the catalog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomisation of magnitude with photometric errors\n",
    "\n",
    "- gaussian distribution should be applied to flux, not magnitudes. Assume central limit theorem applies on magnitudes as well. This could be checked or corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photodet_mag(mag,errmag):\n",
    "    return np.random.normal(loc=mag, scale=errmag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mag_u_det'] = data.apply(lambda x:  photodet_mag(x['mag_u'], x['magerr_u']), axis=1)\n",
    "data['mag_g_det'] = data.apply(lambda x:  photodet_mag(x['mag_g'], x['magerr_g']), axis=1)\n",
    "data['mag_r_det'] = data.apply(lambda x:  photodet_mag(x['mag_r'], x['magerr_r']), axis=1)\n",
    "data['mag_i_det'] = data.apply(lambda x:  photodet_mag(x['mag_i'], x['magerr_i']), axis=1)\n",
    "data['mag_z_det'] = data.apply(lambda x:  photodet_mag(x['mag_z'], x['magerr_z']), axis=1)\n",
    "data['mag_y_det'] = data.apply(lambda x:  photodet_mag(x['mag_y'], x['magerr_y']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection on detected magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def photodet_select(mu,mg,mr,mi,mz,my):\n",
    "    return (mu>17) and (mu < UMAX) and (mg < GMAX) and (mr < RMAX) and (mi < IMAX) and (mz < ZMAX) and (my < YMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['selected'] = data.apply(lambda x:  photodet_select(x['mag_u_det'], x['mag_g_det'], x['mag_r_det'], x['mag_i_det'],x['mag_z_det'], x['mag_y_det'] ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data[\"selected\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('selected', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of average magnitudes after photometric detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magu = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magg = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magr = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magi = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magz = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magy = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout2 = h_magu * maglim_u + h_magg * maglim_g + h_magr * maglim_r + h_magi *  maglim_i + h_magz *  maglim_z + h_magy *  maglim_y\n",
    "    layout2.cols(HV_HISTO_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of detected magnitudes after photometric detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    mag_bin, count = np.histogram(data.mag_u_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magud = hv.Histogram(mag_bin, count).opts(title=f\"mag U\",xlabel=\"U\",color='blue').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_g_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_maggd = hv.Histogram(mag_bin, count).opts(title=f\"mag G\",xlabel=\"G\",color='green').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_r_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magrd = hv.Histogram(mag_bin, count).opts(title=f\"mag R\",xlabel=\"R\",color='red').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_i_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magid = hv.Histogram(mag_bin, count).opts(title=f\"mag I\",xlabel=\"I\",color='orange').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_z_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magzd = hv.Histogram(mag_bin, count).opts(title=f\"mag Z\",xlabel=\"Z\",color='grey').opts(**histo_opts)\n",
    "\n",
    "    mag_bin, count = np.histogram(data.mag_y_det, bins=NBINS_HISTO,range=(mymag0,mymag1))\n",
    "    h_magyd = hv.Histogram(mag_bin, count).opts(title=f\"mag Y\",xlabel=\"Y\",color='black').opts(**histo_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout3 = h_magud * maglim_u + h_maggd * maglim_g + h_magrd * maglim_r + h_magid *  maglim_i + h_magzd *  maglim_z + h_magyd *  maglim_y\n",
    "    layout3.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detected magnitude vs true magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    huudet, xhuz, yhuz=np.histogram2d(data.mag_u,data.mag_u_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hggdet, xhgz, yhgz=np.histogram2d(data.mag_g,data.mag_g_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hrrdet, xhrz, yhrz=np.histogram2d(data.mag_r,data.mag_r_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hiidet, xhiz, yhiz=np.histogram2d(data.mag_i,data.mag_i_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hzzdet, xhzz, yhzz=np.histogram2d(data.mag_z,data.mag_z_det,bins=(50, 50),range=[[16,28],[16,28]])\n",
    "    hyydet, xhyz, yhyz=np.histogram2d(data.mag_y,data.mag_y_det,bins=(50, 50),range=[[16,28],[16,28]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_opts = dict(\n",
    "                #height=600, width=700, \n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                #colorbar=True, toolbar='right', show_grid=True,\n",
    "                #aspect='equal',\n",
    "                frame_width= HV_HISTO_MULTI_FRAME_WIDTH,\n",
    "                show_grid=True ,\n",
    "                #ylabel=\"mag\",\n",
    "                tools=['hover','undo','redo','zoom_in','zoom_out'],\n",
    "                #tools=[myhover,'crosshair'],\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    fhuudet=np.flipud(huudet.T)\n",
    "    fhggdet=np.flipud(hggdet.T)\n",
    "    fhrrdet=np.flipud(hrrdet.T)\n",
    "    fhiidet=np.flipud(hiidet.T)\n",
    "    fhzzdet=np.flipud(hzzdet.T)\n",
    "    fhyydet=np.flipud(hyydet.T)\n",
    "    img_uud=hv.Image(fhuudet, bounds=(16,16,28,28) ).opts(cmap=\"Blues\",title=\"magU det vs magU true \",xlabel=\"U (mag)\",ylabel=\"U (mag)\").opts(**img_opts)\n",
    "    img_ggd=hv.Image(fhggdet, bounds=(16,16,28,28) ).opts(cmap=\"Greens\",title=\"magG det vs magG true\",xlabel=\"G (mag)\",ylabel=\"G (mag)\").opts(**img_opts)\n",
    "    img_rrd=hv.Image(fhrrdet, bounds=(16,16,28,28) ).opts(cmap=\"Reds\",title=\"magR det vs magR true\",xlabel=\"R (mag)\",ylabel=\"R (mag)\").opts(**img_opts)\n",
    "    img_iid=hv.Image(fhiidet, bounds=(16,16,28,28) ).opts(cmap=\"Oranges\",title=\"magI det vs magI true\",xlabel=\"I (mag)\",ylabel=\"I (mag)\").opts(**img_opts)\n",
    "    img_zzd=hv.Image(fhzzdet, bounds=(16,16,28,28) ).opts(cmap=\"Greys\",title=\"magZ det vs magZ true\",xlabel=\"Z (mag)\",ylabel=\"Z (mag)\").opts(**img_opts)\n",
    "    img_yyd=hv.Image(fhyydet, bounds=(16,16,28,28) ).opts(cmap=\"GnBu\",title=\"magY det vs magY true\",xlabel=\"Y (mag)\",ylabel=\"Y (mag)\").opts(**img_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_uud + img_ggd + img_rrd + img_iid + img_zzd + img_yyd\n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average magnitude vs redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    huz, xhuz, yhuz=np.histogram2d(data.redshift,data.mag_u,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hgz, xhgz, yhgz=np.histogram2d(data.redshift,data.mag_g,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hrz, xhrz, yhrz=np.histogram2d(data.redshift,data.mag_r,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hiz, xhiz, yhiz=np.histogram2d(data.redshift,data.mag_i,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hzz, xhzz, yhzz=np.histogram2d(data.redshift,data.mag_z,bins=(50, 50),range=[[0,3],[16,26]])\n",
    "    hyz, xhyz, yhyz=np.histogram2d(data.redshift,data.mag_y,bins=(50, 50),range=[[0,3],[16,26]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_opts = dict(\n",
    "                #height=600, width=700, \n",
    "                xaxis=\"bottom\", \n",
    "                padding = 0.01, fontsize={'title': '8pt'},\n",
    "                #colorbar=True, toolbar='right', show_grid=True,\n",
    "                #aspect='equal',\n",
    "                frame_width= HV_HISTO_MULTI_FRAME_WIDTH,\n",
    "                show_grid=True ,\n",
    "                xlabel=\"redshift\",\n",
    "                #ylabel=\"mag\",\n",
    "                tools=['hover','undo','redo','zoom_in','zoom_out'],\n",
    "                #tools=[myhover,'crosshair'],\n",
    "               )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    fhuz=np.flipud(huz.T)\n",
    "    fhgz=np.flipud(hgz.T)\n",
    "    fhrz=np.flipud(hrz.T)\n",
    "    fhiz=np.flipud(hiz.T)\n",
    "    fhzz=np.flipud(hzz.T)\n",
    "    fhyz=np.flipud(hyz.T)\n",
    "    img_uz=hv.Image(fhuz, bounds=(0,16,3,26) ).opts(cmap=\"Blues\",title=\"magU vs reshift\",ylabel=\"U (mag)\").opts(**img_opts)\n",
    "    img_gz=hv.Image(fhgz, bounds=(0,16,3,26) ).opts(cmap=\"Greens\",title=\"magG vs reshift\",ylabel=\"G (mag)\").opts(**img_opts)\n",
    "    img_rz=hv.Image(fhrz, bounds=(0,16,3,26) ).opts(cmap=\"Reds\",title=\"magR vs reshift\",ylabel=\"R (mag)\").opts(**img_opts)\n",
    "    img_iz=hv.Image(fhiz, bounds=(0,16,3,26) ).opts(cmap=\"Oranges\",title=\"magI vs reshift\",ylabel=\"I (mag)\").opts(**img_opts)\n",
    "    img_zz=hv.Image(fhzz, bounds=(0,16,3,26) ).opts(cmap=\"Greys\",title=\"magZ vs reshift\",ylabel=\"Z (mag)\").opts(**img_opts)\n",
    "    img_yz=hv.Image(fhyz, bounds=(0,16,3,26) ).opts(cmap=\"GnBu\",title=\"magY vs reshift\",ylabel=\"Y (mag)\").opts(**img_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_uz + img_gz + img_rz + img_iz + img_zz + img_yz\n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Color vs redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    humgz, xhumgz, yhumgz=np.histogram2d(data.redshift,data.umg,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hgmrz, xhgmrz, yhgmrz=np.histogram2d(data.redshift,data.gmr,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hrmiz, xhrmiz, yhrmiz=np.histogram2d(data.redshift,data.rmi,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    himzz, xhimzz, yhimzz=np.histogram2d(data.redshift,data.imz,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "    hzmyz, xhzmyz, yhzmyz=np.histogram2d(data.redshift,data.zmy,bins=(50, 50),range=[[0,3],[-0.5,2.]])\n",
    "\n",
    "    fhumgz=np.flipud(humgz.T)\n",
    "    fhgmrz=np.flipud(hgmrz.T)\n",
    "    fhrmiz=np.flipud(hrmiz.T)\n",
    "    fhimzz=np.flipud(himzz.T)\n",
    "    fhzmyz=np.flipud(hzmyz.T)\n",
    "\n",
    "    img_umgz=hv.Image(fhumgz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Blues\",title=\"U - G vs reshift\",ylabel=\"U-G (mag)\").opts(**img_opts)\n",
    "    img_gmrz=hv.Image(fhgmrz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Greens\",title=\"G - R vs reshift\",ylabel=\"G-R (mag)\").opts(**img_opts)\n",
    "    img_rmiz=hv.Image(fhrmiz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Reds\",title=\"R - I vs reshift\",ylabel=\"R-I (mag)\").opts(**img_opts)\n",
    "    img_imzz=hv.Image(fhimzz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Oranges\",title=\"I - Z vs reshift\",ylabel=\"I-Z (mag)\").opts(**img_opts)\n",
    "    img_zmyz=hv.Image(fhzmyz, bounds=(0,-0.5,3,2.) ).opts(cmap=\"Greys\",title=\"Z - Y vs reshift\",ylabel=\"Z-Y (mag)\").opts(**img_opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    layout = img_umgz + img_gmrz + img_rmiz + img_imzz + img_zmyz \n",
    "    layout.cols(HV_IMAGE_MULTI_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because photo-z's rely on the flux integrated in broad filters, they are more sensitive to broad, dramatic features of the SED. This means that the location of these breaks provides a lot of information for photo-z's. \n",
    "- In particular, the Balmer and 4000 angstrom breaks are in the wavelength range of the LSST filters up to a redshift of ~1.4, \n",
    "- and locating these breaks with the LSST filters provides good leverage for photo-z's (see e.g. Kalmbach et al. 2020 and Malz et al. 2021). \n",
    "- Note that while the Balmer break leaves the LSST filters around z=1.4, \n",
    "- the Lyman break doesn't enter the wavelength range of the LSST filters until about z=2.5 :  This gap in redshift coverage contributes to the degradation of photo-z's at high redshift.\n",
    "\n",
    "- Note that in high-redshift galaxies, photo-z estimators might confuse the Lyman break for the Balmer break. This contributes to the physical degeneracies discussed above in \"What makes photo-z's hard?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "if FLAG_SHOW_PHOTOMETRY_DETECTION: \n",
    "    img= Image.open(\"figs/sdss/plot_sdss_filters_2.png\")\n",
    "    img = img.resize((500, 400), Image.ANTIALIAS)\n",
    "    img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Photo-Z estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements from LSST Science book:\n",
    "(https://www.lsst.org/sites/default/files/docs/sciencebook/SB_3.pdf)\n",
    "\n",
    "Photometric redshifts for LSST will be applied and calibrated over the redshift range $0 < z < 4$\n",
    "for galaxies to $r  \\simeq 27.5$. \n",
    "For the majority of science cases, such as weak lensing and BAO, a subset\n",
    "of galaxies with $i < 25.3$ will be used. For this high S/N gold standard subset over the\n",
    "redshift interval, $0 < z < 3$, the photometric redshift requirements are:\n",
    "\n",
    "- The root-mean-square scatter in photometric redshifts, $ \\sigma_z/(1+z)$, must be smaller than 0.05, with a goal of 0.02.\n",
    "- The fraction of $3\\sigma $  outliers at all redshifts must be below 10%.\n",
    "- The bias in $e_z = (z_{photo}−z_{spec})/(1+z_{spec})$ must be below 0.003 (or 0.01 for combined,analyses of weak lensing and baryon acoustic oscillations); \n",
    "- The uncertainty in  $\\sigma_z/(1+z)$ must also be known to similar accuracy.\n",
    "\n",
    "\n",
    "\n",
    "### other definitions\n",
    "\n",
    "- **the photo-z accuracy is the absolute value of the difference between the true and photometric redshifts**.\n",
    "\n",
    "-  **the photo-z uncertainty is the standard deviation of the true redshifts** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "\n",
    "\n",
    "- from DE School IV, University of Oxford, July 18, 2016 :  **Jeff Newman - photometric redshifts for LSST**\n",
    "\n",
    "The tools for PhotoZ evaluation are givenin the notebook and also described in the LSST science book (Performance Chapter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances Evaluation lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that we will call a lot: makes the zphot/zspec plot and calculates key statistics\n",
    "def plot_lines(zmin=0,zmax=3,zstep=0.05,slope=0.15):\n",
    "    \n",
    "    x = np.arange(zmin,zmax,zstep)\n",
    "    outlier_upper = x + slope*(1+x)\n",
    "    outlier_lower = x - slope*(1+x)\n",
    "\n",
    "    curv_bisect=hv.Curve(zip(x,x)).opts(color=\"red\") \n",
    "    curv_outupper=hv.Curve(zip(x,outlier_upper)).opts(color=\"red\",line_dash='dashed') \n",
    "    curv_outlower=hv.Curve(zip(x,outlier_lower)).opts(color=\"red\",line_dash='dashed') \n",
    "    \n",
    "    layout = curv_bisect * curv_outupper * curv_outlower\n",
    "    return layout\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(z_spec,z_phot,slope=0.15):\n",
    "    \"\"\"\n",
    "    input : \n",
    "       - z_spec : spectroscopic redshift or true redshift\n",
    "       - z_phot : photo-z reedshift\n",
    "       - slope : slope of line defining the outliers  3 x sigma_z with sigma_z = 5%, so slope = 3 x 0.05 = 0.15 \n",
    "    \"\"\"\n",
    "    \n",
    "    mask = np.abs((z_phot - z_spec)/(1 + z_spec)) > slope\n",
    "    notmask = ~mask \n",
    "    \n",
    "    # Standard Deviation of the predicted redshifts compared to the data:\n",
    "    #-----------------------------------------------------------------\n",
    "    std_result = np.std((z_phot - z_spec)/(1 + z_spec), ddof=1)\n",
    "    print('Standard Deviation: %6.4f' % std_result)\n",
    "    \n",
    "\n",
    "    # Normalized MAD (Median Absolute Deviation):\n",
    "    #------------------------------------------\n",
    "    nmad = 1.48 * np.median(np.abs((z_phot - z_spec)/(1 + z_spec)))\n",
    "    print('Normalized MAD: %6.4f' % nmad)\n",
    "\n",
    "    # Percentage of delta-z > 0.15(1+z) outliers:\n",
    "    #-------------------------------------------\n",
    "    eta = np.sum(np.abs((z_phot - z_spec)/(1 + z_spec)) > 0.15)/len(z_spec)\n",
    "    print('Delta z >0.15(1+z) outliers: %6.3f percent' % (100.*eta))\n",
    "    \n",
    "    # Median offset (normalized by (1+z); i.e., bias:\n",
    "    #-----------------------------------------------\n",
    "    bias = np.median(((z_phot - z_spec)/(1 + z_spec)))\n",
    "    sigbias=std_result/np.sqrt(0.64*len(z_phot))\n",
    "    print('Median offset: %6.3f +/- %6.3f' % (bias,sigbias))\n",
    "    \n",
    "    \n",
    "     # overlay statistics with titles left-aligned and numbers right-aligned\n",
    "    stats_txt = '\\n'.join([\n",
    "        'NMAD  = {:0.2f}'.format(nmad),\n",
    "        'STDEV = {:0.2f}'.format(std_result),\n",
    "        'BIAS  = {:0.2f}'.format(bias),\n",
    "        'ETA   = {:0.2f}'.format(eta)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return nmad,std_result,bias,eta,stats_txt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## START ML here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and Feature\n",
    "\n",
    "Because we want to estimate the performance of photoz estimator itself, not the total performance including intrinsic redshift fluctuations. Thus only average magnitudes data will be used : detected magnitude are dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[\"redshift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[[\"mag_u\",\"mag_g\",\"mag_r\",\"mag_i\",\"mag_z\",\"mag_y\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Total number of samples to split in training, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntot = len(target)\n",
    "Ntot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in training / test set\n",
    "\n",
    "- speed of the notebook must be tuned with the training sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of samples to be used in training\n",
    "\n",
    "- depending on the required speed of the demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 5000\n",
    "Ntest = Ntot-Ntrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "test_sample_size_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note**\n",
    "- because the model fit (training) may be long, we should limit the training dataset size for this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressors definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear model\n",
    "\n",
    "- Instead of using the LinearRegressor, we start by using the regularized Ridge regressor with the alpha parameter setting the regularization.\n",
    "- Linear model features should be always normalized,\n",
    "- For non linearities, we include the possibility to develop the model as a polynomial of features\n",
    "\n",
    "Scikit-Learn offer to define pipelines of tasks in an easy way:\n",
    "- PolynomialFeatures() task extend features dataset in powers of thos features up to a power degree,\n",
    "- StandardScaler() preprocess the features to normalize them,\n",
    "- Ridge is the regularized version of the LinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_regressor = make_pipeline(PolynomialFeatures(degree=5), StandardScaler(),Ridge(alpha=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_regressor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest regressor\n",
    "\n",
    "- RandomForest regressor is an Enssemble regressor of type Bagging (Bootstrap and Aggregation).  \n",
    "\n",
    "- RandomForest regressor combines the regression of multiple decision tree regressors fitted in parallel \n",
    "on bootstrapped samples from the training sample.\n",
    "\n",
    "- Each individual decision tree is deep, meaning they individually overfit the bootstrapped samples. \n",
    "\n",
    "- The aggregation of the overfitting parallel decision tree model reduce the over-fitting.\n",
    "- For Random Forest, each Decision Tree node feature are drawn randomly. This reduce the error correlation of the various trees.\n",
    "- From this caracteristics, RandomForest is expected to be one of the best non-linear regressor on column-tabulated datasets.\n",
    "\n",
    "\n",
    "- RandomForst includes a number of hyper-parameters.\n",
    "- We use the hyper-parameters chosen byJeff Newmann for the DE-School at Oxford 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "randomforest_regressor = RandomForestRegressor(n_estimators = 50, max_depth = 30, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor\n",
    "\n",
    "The INRIA MOOC (2022) on scikit-Learn ( Machine learning in Python with scikit-learn: https://lms.fun-mooc.fr/courses/course-v1:inria+41026+session02/info)\n",
    "recommend the histogram-binned version of GradientBoostingRegressor, expected to have a good balance between underfitting and overfitting.\n",
    "\n",
    "- Boosting Regressor perform shallow Decision trees (underfitting) fit sequencially. Among them, Gradient Boosting Regressor are expected to avoid overfitting. Among them the Histogram Grandient Boosting regressor is expected to run faster    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "#from sklearn.preprocessing import KBinsDiscretizer\n",
    "#discretizer = KBinsDiscretizer(n_bins=64, encode=\"ordinal\", strategy=\"quantile\")\n",
    "#histogram_gradient_boosting_regressor = make_pipeline(discretizer, HistGradientBoostingRegressor(max_iter=30))\n",
    "histogram_gradient_boosting_regressor =  HistGradientBoostingRegressor(max_iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = {'r2': make_scorer(r2_score),'mae': make_scorer(mean_absolute_error),'mse': make_scorer(mean_squared_error)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use of cross-validation\n",
    "\n",
    "- We use cross-validation to select a sub-sample of galaxies from the complete training sample and train the model with this subset.\n",
    "- This sub-sampling is repeated several times (n_split=5).\n",
    "\n",
    "The interest of this multi-subsampling is to have a set of almost similar but slightly different fitted models from which we can derive several predictions for a test sample, thus an average predicted value and its variation (or a PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't know of the galaxies are ordered by redshift or come randomly. Thus we activate a pre-random-shuffling in the training dataset. \n",
    "cv = ShuffleSplit(n_splits=5, test_size=.80, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge model\n",
    "\n",
    "- The cross_validate function performs fit on n_splits models from n_splits random subsamples\n",
    "- The smaple is previously randomized\n",
    "- The evaluation metric is given by the scoring (The INRIA MOOC use this coring=\"neg_mean_absolute_error\"),\n",
    "- The n_splits fitted models are retuned (to be able to make n_split prediction for a single test sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters names\n",
    "# ridge_regressor.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation curve on the regularisation hyperparameter alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "the_degree = 2\n",
    "the_title  = f\"Ridge regressor : Validation curve with degree = {the_degree}\"\n",
    "ridge_regressor = make_pipeline(PolynomialFeatures(degree=the_degree), StandardScaler(),Ridge(alpha=0.05))\n",
    "\n",
    "alpha_grid = np.array([ 0.0001, 0.0005, 0.001, 0.005,  0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10., 50. ])\n",
    "train_scores, test_scores = validation_curve(\n",
    "    ridge_regressor, X_train, y_train, param_name=\"ridge__alpha\", param_range=alpha_grid,\n",
    "    cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curv_train=hv.Curve(zip(np.log10(alpha_grid) , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"log(alpha)\",ylabel=\"error\",xlim=(-4,2)) \n",
    "curv_test=hv.Curve(zip(np.log10(alpha_grid) , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"log(alpha)\",ylabel=\"error\",xlim=(-4,2)) \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,title=the_title,legend_position='bottom_right',ylim=(0,1.1*test_errors.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on validation curve for alpha:\n",
    "- high alpha give high bias on both training and testing validation curve (right of the figure)\n",
    "- low alpha means no regularisation. Training and testing validation curves are diverging. Error on training are decreasing (training overfitting) while error on testing is increasing (left of the figure).  \n",
    "- A good parameter for alpha is between [0.1 , 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation curve on the  hyperparameter : degree of the polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "the_alpha = 0.01\n",
    "the_degree = 2\n",
    "the_title  = f\"Ridge regressor : Validation curve with alpha = {the_alpha}\"\n",
    "ridge_regressor = make_pipeline(PolynomialFeatures(degree=the_degree), StandardScaler(),Ridge(alpha=the_alpha))\n",
    "\n",
    "degree_grid = np.arange(1,12)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    ridge_regressor, X_train, y_train, param_name=\"polynomialfeatures__degree\", param_range=degree_grid,\n",
    "    cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curv_train=hv.Curve(zip(degree_grid , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"degree\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(degree_grid , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"degree\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,title=the_title,legend_position='bottom_right',ylim=(0,1.1*test_errors.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on validation curve for degree:\n",
    "- low degree means too simple model (left of the figure) means high bias for both training and testing validation curve.\n",
    "- high degree means means too complex model (right of the figure). Training and testing validation curves are diverging. Error on training are decreasing (training overfitting) while error on testing is increasing (left of the figure).  \n",
    "- A good parameter for degree is 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 10000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=np.arange(50,Ntrain//5,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "the_alpha = 0.1\n",
    "the_degree = 2\n",
    "the_title  = f\"Ridge regressor : Learning curve : degree={the_degree} , alpha = {the_alpha}\"\n",
    "\n",
    "ridge_regressor = make_pipeline(PolynomialFeatures(degree=the_degree), StandardScaler(),Ridge(alpha=the_alpha))\n",
    "\n",
    "results = learning_curve(ridge_regressor, X_train, y_train, train_sizes=train_sizes, cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "#results = learning_curve(ridge_regressor, X_train, y_train, cv=cv,\n",
    "#    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "# Convert the scores into errors\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)),np.sqrt( -test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curv_train=hv.Curve(zip(train_sizes , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(train_sizes , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,legend_position='bottom_right',title=the_title,ylim=(0,1.1*test_errors.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on leaning curve:\n",
    "\n",
    "- Linear model require few samples for learning.\n",
    "- Training and testing learning curve converge at about n_sample=1000, with no error gap between the two curve.\n",
    "- This means the model is good without under-fitting nor overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search Cross Validation\n",
    "\n",
    "- The goal is to find the optimum pair of hyper parameters by a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the parameter names\n",
    "#ridge_regressor.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the grid search of hyper parameters\n",
    "param_grid = {'polynomialfeatures__degree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 'ridge__alpha': [0.001, .01, .1, 1., 10. ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_ridge = GridSearchCV(estimator=ridge_regressor, param_grid=param_grid, scoring=\"neg_mean_squared_error\" ,n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitation of the number of samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 1000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit of the best hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t1 = datetime.datetime.now()\n",
    "grid_search_ridge.fit(X_train, y_train)\n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Ridge GridSearchCV : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameters found are: {grid_search_ridge.best_params_}\")\n",
    "print(f\"The best score: {grid_search_ridge.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_search_ridge.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(grid_search_ridge.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.rename(columns={\"param_polynomialfeatures__degree\":\"degree\",\"param_ridge__alpha\":\"alpha\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = df_cv_results[[\"degree\",\"alpha\",\"mean_test_score\",\"std_test_score\",\"rank_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results[\"mean_test_score\"] = np.sqrt(- df_cv_results[\"mean_test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.sort_values(by=\"rank_test_score\", axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "# Too long for a live demo !!!\n",
    "# cv_results = cross_validate(grid_search_ridge, X_train, y_train, cv=3, n_jobs=2, scoring = \"neg_mean_squared_error\", return_estimator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the n_splits model, but all predictions for all estimators could be calculated (average and rms)\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt1 = get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a holoviews object to hold and plot data\n",
    "# Create the linked streams instance\n",
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p1 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p1 = p1.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Optimum Ridge Regressor (grid CV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 * plot_lines() * hv.Text(0.5, 2.5, stats_txt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "class loguniform_int:\n",
    "    \"\"\"Integer valued version of the log-uniform distribution\"\"\"\n",
    "    def __init__(self, a, b):\n",
    "        self._distribution = loguniform(a, b)\n",
    "\n",
    "    def rvs(self, *args, **kwargs):\n",
    "        \"\"\"Random variable sample\"\"\"\n",
    "        return self._distribution.rvs(*args, **kwargs).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    'polynomialfeatures__degree': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'ridge__alpha': loguniform(0.001, 10),\n",
    "}\n",
    "\n",
    "random_search_ridge  = RandomizedSearchCV(\n",
    "    ridge_regressor, param_distributions=param_distributions, n_iter=20,\n",
    "    cv=5, verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t1 = datetime.datetime.now()\n",
    "random_search_ridge.fit(X_train, y_train)\n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Ridge RandomSearchCV : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameters found are: {random_search_ridge.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best score: {random_search_ridge.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = random_search_ridge.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(random_search_ridge.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.rename(columns={\"param_polynomialfeatures__degree\":\"degree\",\"param_ridge__alpha\":\"alpha\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = df_cv_results[[\"degree\",\"alpha\",\"mean_test_score\",\"std_test_score\",\"rank_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.sort_values(by=\"rank_test_score\", axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt1b = get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a holoviews object to hold and plot data\n",
    "# Create the linked streams instance\n",
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p1b = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p1b = p1b.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Optimum Ridge Regressor (random CV)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1b * plot_lines() * hv.Text(0.5, 2.5, stats_txt1b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select smaller training sample\n",
    "\n",
    "- faster model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 5000\n",
    "Ntest = Ntot-Ntrain\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_n_estimators = 50\n",
    "the_max_depth = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_regressor = RandomForestRegressor(n_estimators = the_n_estimators, max_depth = the_max_depth, max_features = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "n_estimators_grid = range(0,50,5)\n",
    "max_depth_grid = range(1,30)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    randomforest_regressor, X_train, y_train, param_name=\"n_estimators\", param_range=n_estimators_grid ,\n",
    "    cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_title = f\"Random Forest regressor : Validation curve : max_depth = {the_max_depth}\"\n",
    "curv_train=hv.Curve(zip(n_estimators_grid  , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"n_estimators\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(n_estimators_grid  , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"n_estimators\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,title=the_title,ylim=(0,test_errors.max()*1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on validation curve for n_estimators:\n",
    "- Error not very sensitive with n_estimators\n",
    "- Small overfit on training sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "n_estimators_grid = range(0,50,5)\n",
    "max_depth_grid = range(1,30)\n",
    "\n",
    "train_scores, test_scores = validation_curve(\n",
    "    randomforest_regressor, X_train, y_train, param_name=\"max_depth\", param_range=max_depth_grid ,\n",
    "    cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_title = f\"Random Forest regressor : Validation curve : n_estimators = {the_n_estimators}\"\n",
    "curv_train=hv.Curve(zip(max_depth_grid  , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"max_depth\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(max_depth_grid  , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"max_depth\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,title=the_title,ylim=(0,test_errors.max()*1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on validation curve for max_depth:\n",
    "- at low max_depth, high bias on training and test set\n",
    "- above few max_depth , overfitting on training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 10000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=np.arange(10,Ntrain//5,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "randomforest_regressor = RandomForestRegressor(n_estimators = the_n_estimators, max_depth = the_max_depth, max_features = 'auto')\n",
    "\n",
    "results = learning_curve(randomforest_regressor, X_train, y_train, train_sizes=train_sizes, cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "#results = learning_curve(randomforest_regressor, X_train, y_train, cv=cv,\n",
    "#    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "# Convert the scores into errors\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_title = f\"Random Forest regressor : Learning curve  n_estimators={the_n_estimators} , max_depth = {the_max_depth}\"\n",
    "curv_train=hv.Curve(zip(train_sizes , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(train_sizes , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=700,height=300,title=the_title,ylim=(0,test_errors.max()*1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on leaning curve:\n",
    "\n",
    "- Training and testing learning curve converge at about n_sample=1000, with a residual error gap between the two curve.\n",
    "- This means the model is slightly overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search cross- Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 2000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest_regressor.get_params() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': [10, 20, 30, 40, 50], 'max_depth': [2,5,10,15,20,25,30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_randomforest = GridSearchCV(estimator=randomforest_regressor, param_grid=param_grid, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t1 = datetime.datetime.now()\n",
    "grid_search_randomforest.fit(X_train, y_train)\n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Random Forest GridSearchCV : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameters found are: {grid_search_randomforest.best_params_}\")\n",
    "print(f\"The best score: {grid_search_randomforest.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator = grid_search_randomforest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = pd.DataFrame(grid_search_randomforest.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.rename(columns={\"param_max_depth\":\"max_depth\",\"param_n_estimators\":\"n_estimators\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results = df_cv_results[[\"max_depth\",\"n_estimators\",\"mean_test_score\",\"std_test_score\",\"rank_test_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_results.sort_values(by=\"rank_test_score\", axis=0, ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the n_splits model, but all predictions for all estimators could be calculated (average and rms)\n",
    "y_pred = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt2= get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p2 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p2 = p2.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Optimum Random Forest Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 * plot_lines() *  hv.Text(0.5, 2.5, stats_txt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Gradient Boosting regressor\n",
    "\n",
    "- No particular optimisation of hyper parameters performed here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation curve on max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 5000\n",
    "Ntest = Ntot-Ntrain\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_gradient_boosting_regressor = HistGradientBoostingRegressor(random_state=0)\n",
    "max_iter = [1, 2, 5, 10, 20, 50, 100, 200, 500]\n",
    "train_scores_hgbdt, test_scores_hgbdt = validation_curve(\n",
    "    histogram_gradient_boosting_regressor, X_train, y_train, param_name=\"max_iter\", param_range=max_iter, scoring=\"neg_mean_squared_error\",cv=cv, n_jobs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = np.sqrt(-train_scores_hgbdt.mean(axis=1))\n",
    "test_error  = np.sqrt(-test_scores_hgbdt.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curv_train=hv.Curve(zip(max_iter  , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"max_iter\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(max_iter  , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"max_iter\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=600,height=300,title=\"Histogram gradient boosting : Validation curve\",ylim=(0,test_errors.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 10000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes=np.arange(10,Ntrain//5,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_max_iter = 30\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "histogram_gradient_boosting_regressor = HistGradientBoostingRegressor(max_iter=the_max_iter,random_state=0)\n",
    "\n",
    "results = learning_curve(histogram_gradient_boosting_regressor, X_train, y_train, train_sizes=train_sizes, cv=cv,\n",
    "    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "#results = learning_curve(histogram_gradient_boosting_regressor, X_train, y_train, cv=cv,\n",
    "#    scoring=\"neg_mean_squared_error\", n_jobs=2)\n",
    "\n",
    "train_size, train_scores, test_scores = results[:3]\n",
    "# Convert the scores into errors\n",
    "train_errors, test_errors = np.sqrt(-train_scores.mean(axis=1)), np.sqrt(-test_scores.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_title = f\"Histogram Gradient Boosting : Learning curve  : max_iter = {the_max_iter}\"\n",
    "curv_train=hv.Curve(zip(train_sizes , train_errors),label=\"train\").opts(color=\"blue\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "curv_test=hv.Curve(zip(train_sizes , test_errors),label=\"test\").opts(color=\"red\",xlabel=\"n_train\",ylabel=\"error\") \n",
    "layout = (curv_train * curv_test).opts(width=700,height=300,title=the_title,ylim=(0,test_errors.max()*1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### comments on leaning curve:\n",
    "\n",
    "- Training and testing learning curve converge very slowly a with a residual error gap between the two curve.\n",
    "- This means the model is slightly overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discretizer = KBinsDiscretizer(n_bins=64, encode=\"ordinal\", strategy=\"quantile\")\n",
    "#histogram_gradient_boosting_regressor = make_pipeline(discretizer, HistGradientBoostingRegressor(max_iter=30))\n",
    "histogram_gradient_boosting_regressor = HistGradientBoostingRegressor(max_iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_iter': range(10,60,10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_gradient_boosting = GridSearchCV(estimator=histogram_gradient_boosting_regressor, param_grid=param_grid, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit the number ov samples for optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 2000\n",
    "Ntest = Ntot-Ntrain\n",
    "# Test fraction\n",
    "test_sample_size_fraction=Ntest/Ntot\n",
    "\n",
    "# adapt the train dataset size according required running time \n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=test_sample_size_fraction, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV grid search training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We simply use the fit method, not the cross_validate to accelerate the demo \n",
    "t1 = datetime.datetime.now()\n",
    "grid_search_gradient_boosting.fit(X_train,y_train) \n",
    "t2 = datetime.datetime.now()\n",
    "deltat = (t2-t1).total_seconds() \n",
    "print(f\"Histogram Gradient Boosting GridSearchCV : elapsed time {deltat:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The best parameters found are: {grid_search_gradient_boosting.best_params_}\")\n",
    "print(f\"The best score: {grid_search_gradient_boosting.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimator=grid_search_gradient_boosting.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =  best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = zip(y_test,y_pred)\n",
    "points = hv.Points(coords).opts(tools=['box_select', 'lasso_select'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmad,std_result,bias,eta,stats_txt3 = get_stats(y_test.values,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundsxy = (0, 0, 0, 0)\n",
    "box = streams.BoundsXY(source=points, bounds=boundsxy)\n",
    "bounds = hv.DynamicMap(lambda bounds: hv.Bounds(bounds), streams=[box])\n",
    "\n",
    "# Apply the datashader\n",
    "p3 = dynspread(datashade(points, cmap=\"Viridis\"))\n",
    "p3 = p3.opts(width=HV_HISTO_SINGLE_WIDTH, height=HV_HISTO_SINGLE_HEIGHT,\n",
    "    padding=0.05, show_grid=True,\n",
    "    xlim=(0, 3), ylim=(0, 3.0),\n",
    "    xlabel=\"z-spec\", ylabel=\"z-phot\",title=\"Optimum Histogram Gradient Boosting Regressor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 * plot_lines() *  hv.Text(0.5, 2.5, stats_txt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance metrics in scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2  = r2_score(y_pred,y_test)\n",
    "mae = mean_absolute_error(y_pred,y_test)\n",
    "mse = mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_r2   = f\"R2 score : \\t\\t {r2:.3f}\"\n",
    "msg_mae  = f\"MAE mean absolute error : \\t {mae:.3f}\"\n",
    "msg_rmsq = f\"Root MSE error : \\t\\t {np.sqrt(mse):.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(msg_r2)\n",
    "print(msg_mae)\n",
    "print(msg_rmsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work : continue optimisation\n",
    "\n",
    "The bias is OK, but the shape and the scatter is not that good.\n",
    "\n",
    "- Play with other hyper-parameters to improve Random Forest and Hist Gradient Boosting estimators,\n",
    "- Try other estimators SVM, Neural networks,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the output directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_CLEAN_DATAONDISK:\n",
    "    if os.path.isdir(TMPNBDIR):\n",
    "        try:\n",
    "            shutil.rmtree(TMPNBDIR)\n",
    "        except OSError as e:\n",
    "            print(\"Error: %s : %s\" % (TMPNBDIR, e.strerror)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
